# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from .model import Model
from .executor import Executor
from .tensor import Tensor
from ._ark_core import _init, TensorType
import logging
import numpy as np

# Use a global variable to track the state of the ARK runtime
ArkRuntimeState = ("start", "init_model", "launch", "run", "stop", "destroy")


class Config:
    global_config = None

    def __init__(self, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        self.ark_runtime_state = "start"


def init():
    """
    Init an ARK program. Call this function to clean up the shared memory
    directory. This is useful when the previous run crashed, as this forces
    to remove locks generated by previous runs. This may crash other ARK
    processes running on the same machine, if there are any.
    """
    Config.global_config = None
    Model.global_model = None
    Executor.global_executor = None
    _init()


def init_model(rank: int = 0, world_size: int = 1):
    """
    Initialize the ARK runtime and create a global model that will record
    all the operations.
    """
    if Config.global_config is not None:
        logging.warn("ARK runtime is already initialized, skip init_model")
        return
    Config.global_config = Config(rank, world_size)
    Model.global_model = Model(rank)
    Config.global_config.ark_runtime_state = "init_model"


def launch():
    """
    Create an executor and schedule the ARK model. The scheduler will generate
    the CUDA kernels. The GPU context and the connection between GPUs will be
    initialized. The executor will compile the cuda kernels and launch the ARK runtime.
    """
    if (
        Config.global_config.ark_runtime_state != "init_model"
        and Config.global_config.ark_runtime_state != "stop"
    ):
        logging.warn(
            "ARK runtime is not initialized or already launched, skip launching"
        )
        return
    Config.global_config.ark_runtime_state = "launch"
    rank = Config.global_config.rank
    world_size = Config.global_config.world_size
    Executor.global_executor = Executor(
        rank, rank, world_size, Model.get_global_model(), "Executor"
    )
    Executor.get_global_executor().compile()
    Executor.get_global_executor().launch()


def run(iter=1, async_run=False):
    """
    Run the ARK program for iter iterations and wait for the kernel to finish.
    """
    if Config.global_config.ark_runtime_state != "launch":
        logging.error("ARK runtime is not launched")
        raise RuntimeError("ARK runtime is not launched")
    Config.global_config.ark_runtime_state = "run"
    Executor.get_global_executor().run(iter)
    if not async_run:
        stop()


def wait():
    """
    Wait for the kernel to finish.
    """
    if Config.global_config.ark_runtime_state != "run":
        logging.warn("ARK runtime is not running, skip waiting")
        return
    Executor.get_global_executor().wait()
    Config.global_config.ark_runtime_state = "launch"


def stop():
    """
    Stop the model and return the elapsed time in milliseconds.
    Once this is called, we need to call `launch()` again to run the model again.
    """
    if (
        Config.global_config.ark_runtime_state != "run"
        and Config.global_config.ark_runtime_state != "launch"
    ):
        logging.warn("ARK runtime is not running or launched, skip stopping")
        return
    Executor.get_global_executor().stop()
    Config.global_config.ark_runtime_state = "stop"


def destroy():
    """
    Destroy the ARK runtime and release all the resources.
    """
    if (
        Config.global_config.ark_runtime_state == "run"
        or Config.global_config.ark_runtime_state == "launch"
    ):
        stop()
    Config.global_config.ark_runtime_state = "destroy"
    Executor.global_executor = None
    Model.global_model = None
    Config.global_config = None


def tensor_memcpy_host_to_device(dst: Tensor, src: np.ndarray):
    """
    Copy a tensor from host to device. Used for initializing the tensor on device.
    """
    # Check the current ARK runtime status
    if (
        Config.global_config.ark_runtime_state != "launch"
        and Config.global_config.ark_runtime_state != "stop"
    ):
        logging.error("ARK runtime is not launched")
        raise RuntimeError("ARK runtime is not launched")
    Executor.get_global_executor().tensor_memcpy_host_to_device(
        dst._tensor, src
    )
    return dst


def tensor_memcpy_device_to_host(dst: np.ndarray, src: Tensor):
    """
    Copy a tensor from device to host. If dst is None, a new numpy array will be created.
    """
    # Check the current ARK runtime status
    if (
        Config.global_config.ark_runtime_state != "launch"
        and Config.global_config.ark_runtime_state != "stop"
    ):
        logging.error("ARK runtime is not launched")
        raise RuntimeError("ARK runtime is not launched")
    # Create a new numpy array if dst is None
    if dst is None:
        np_type = None
        if src.tensor_type() == TensorType.FP32:
            np_type = np.float32
        elif src.tensor_type() == TensorType.FP16:
            np_type = np.float16
        else:
            logging.error("Unsupported tensor type")
            raise TypeError("Unsupported tensor type")
        dst = np.empty(src.shape, dtype=np_type)
    Executor.get_global_executor().tensor_memcpy_device_to_host(
        dst, src._tensor
    )
    return dst
