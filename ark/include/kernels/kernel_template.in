// THIS KERNEL IS MACHINE-GENERATED BY ARK.
#define ARK_WARPS_PER_BLOCK @NUM_WARPS_PER_BLOCK@
#include "ark_kernels.h"
using namespace ark;

template <size_t ProcBegin, size_t ProcEnd, size_t ProcStep, size_t ProcCurrent,
          size_t TaskBegin, size_t TaskEnd, size_t TaskStep, size_t TaskGranularity,
          size_t NumSlots, size_t SlotNumWarps, size_t SlotSramBytes,
          void (*task)(char*, int, int, @GLOBAL_ARGS@)>
__forceinline__ __device__ void task_seq(char *_buf, @GLOBAL_ARGS@) {
  if (math::geq<ProcBegin>(blockIdx.x) && math::le<ProcEnd>(blockIdx.x) &&
      ((blockIdx.x - ProcBegin) % ProcStep == 0)) {
    constexpr size_t SlotNumThreads = SlotNumWarps * Arch::ThreadsPerWarp;
    constexpr size_t NumProcs = (ProcEnd - ProcBegin + ProcStep - 1) / ProcStep;
    constexpr size_t SramBytesPerWarp = SlotSramBytes / SlotNumWarps;
    size_t p = ((blockIdx.x + gridDim.x - ProcCurrent) % gridDim.x) / ProcStep;
    size_t k = threadIdx.x / SlotNumThreads;
    if constexpr (ARK_WARPS_PER_BLOCK > SlotNumWarps) {
      if (k >= NumSlots) return;
    }
    size_t task_id_base = TaskBegin + p * TaskStep * TaskGranularity;
    for (size_t t = k; ; t += NumSlots) {
      size_t task_id = task_id_base + TaskStep *
        (t % TaskGranularity + t / TaskGranularity * TaskGranularity * NumProcs);
      if (task_id >= TaskEnd) break;
      task(_buf, task_id, SramBytesPerWarp, @GLOBAL_ARGS@);
    }
  }
}

__device__ int ARK_ITER;
__device__ sync::State ARK_LOOP_SYNC_STATE;

@DEFINITIONS@

__device__ void ark_body(char *_buf, int _iter, @GLOBAL_ARGS@) {
@BODY@
}

extern "C" __global__ __launch_bounds__(ARK_WARPS_PER_BLOCK * Arch::ThreadsPerWarp, 1)
void ark_loop_kernel@NAME@(char *_buf, int *_iter, @GLOBAL_ARGS@) {
  int *shared_mem = (int *)_ARK_SMEM;
  for (int i = threadIdx.x; i < ARK_SMEM_RESERVED_BYTES / sizeof(int); i += blockDim.x) {
    shared_mem[i] = 0;
  }
  for (;;) {
    if (threadIdx.x == 0 && blockIdx.x == 0) {
      int iter;
      while ((iter = atomicLoadRelaxed(_iter)) == 0) {}
      ARK_ITER = iter;
    }
    sync_gpu<@NUM_BLOCKS@>(ARK_LOOP_SYNC_STATE);
    if (ARK_ITER < 0) return;

    ark_body(_buf, 0, @GLOBAL_ARGS@);
    for (int _i = 1; _i < ARK_ITER; ++_i) {
      sync_gpu<@NUM_BLOCKS@>(ARK_LOOP_SYNC_STATE);
      ark_body(_buf, _i, @GLOBAL_ARGS@);
    }
    if (threadIdx.x == 0) {
      __threadfence_system();
    }
    sync_gpu<@NUM_BLOCKS@>(ARK_LOOP_SYNC_STATE);
    if (threadIdx.x == 0 && blockIdx.x == 0) {
      atomicStoreRelaxed(_iter, 0);
    }
    sync_gpu<@NUM_BLOCKS@>(ARK_LOOP_SYNC_STATE);
  }
}

extern "C" __global__ __launch_bounds__(ARK_WARPS_PER_BLOCK * Arch::ThreadsPerWarp, 1)
void ark_kernel@NAME@(int _iter) {
  int *shared_mem = (int *)_ARK_SMEM;
  for (int i = threadIdx.x; i < ARK_SMEM_RESERVED_BYTES / sizeof(int); i += blockDim.x) {
    shared_mem[i] = 0;
  }
  ark_body(_buf, _iter, @GLOBAL_ARGS@);
}
