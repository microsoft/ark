// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.
#include <cassert>

#include "ark/logging.h"
#include "ark/math.h"
#include "ark/model_io.h"

using namespace std;

namespace ark {

// Shard `input` along `axis` into `dim_per_shard`-dimensional shards.
vector<Tensor *> Model::sharding(Tensor *input, DimType axis,
                                 DimType dim_per_shard, const string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "sharding ", input->shape, " ", axis, " ", dim_per_shard);
    if (axis >= DIMS_LEN) {
        LOGERR("invlaid axis value: ", axis);
    }
    if ((input->shape[axis] % dim_per_shard) != 0) {
        // If the total dimension is not divided by the per-shard size,
        // we need to check whether we can put a padding here.
        // If the padded dimension of the input tensor is smaller than
        // the leading dimension size, it means that the input tensor refers to
        // a part of a buffer -- in this case, we cannot put a padding because
        // the tensor has adjacent data.
        DimType pdim = math::pad(input->shape[axis], input->pads[axis]);
        if (pdim < input->ldims[axis]) {
            LOGERR("the dimension of axis ", axis, " (", input->shape[axis],
                   ") is not divided by the dimension per shard (",
                   dim_per_shard, ") and this tensor cannot be padded.");
        }
    }
    vector<Tensor *> shards;
    DimType num_shard = math::div_up(input->shape[axis], dim_per_shard);
    Dims shard_shape = input->shape;
    Dims shard_offs = input->offs;
    Dims shard_pads = input->pads;
    for (DimType i = 0; i < num_shard; ++i) {
        DimType dim;
        if (i == (num_shard - 1)) {
            dim = input->shape[axis] - (i * dim_per_shard);
            shard_pads[axis] = input->pads[axis];
        } else {
            dim = dim_per_shard;
            shard_pads[axis] = 1;
        }
        shard_shape[axis] = dim;
        Tensor *shard = this->identity(
            this->tensor(shard_shape, input->type, input->buf, input->ldims,
                         shard_offs, shard_pads),
            {input}, nullptr, name + "/identity_" + to_string(i));
        shards.emplace_back(shard);
        shard_offs[axis] += dim;
    }
    return shards;
}

// Create a new TensorBuf object with `bytes` bytes.
// A common usage is setting `bytes` to 0 during declaring a model and let the
// scheduler determine the value after the model is completely defined.
TensorBuf *Model::create_tensor_buf(const DimType bytes)
{
    TensorBuf *buf = new TensorBuf{bytes, (int)this->tns_bufs_storage.size()};
    assert(buf != nullptr);
    this->tns_bufs_storage.emplace_back(buf);
    return buf;
}

// Remove a TensorBuf object from the model.
void Model::destroy_tensor_buf(const TensorBuf *buf)
{
    for (auto &tns : this->tns_storage) {
        if (tns->buf == buf) {
            LOGERR("dangling tensor detected");
        }
    }
    bool is_found = false;
    auto it = this->tns_bufs_storage.begin();
    for (; it != this->tns_bufs_storage.end(); ++it) {
        if (it->get() == buf) {
            this->tns_bufs_storage.erase(it);
            is_found = true;
            break;
        }
    }
    if (!is_found) {
        LOGERR("the given TensorBuf is not found");
    }
}

// Add a new operator to the model. This is a helper function for operator APIs.
//
// `type`: the type of the operator
// `prec_type`: the precision type of the operator
// `in_deps`:
//     The input tensors of the operator, including execution dependencies.
// `out_deps`:
//     The output tensors of the operator, including execution dependencies.
// `args`:
//     The arguments of the operator.
// `name`: the name of the operator
// `gran_lev`:
//      The granularity level of the operator. Larger values should indicate
//      finer-grained operators. If it is -1, the granularity level will be
//      automatically determined by the scheduler.
//
Op *Model::create_op(const OpType &type, const OpPrecType &prec_type,
                     const vector<Tensor *> &in_deps,
                     const vector<Tensor *> &out_deps,
                     const vector<OpArg> &args, const string &name,
                     int gran_lev)
{
    string suffix_str;
    auto p = this->name_cnts.emplace(name, 1);
    if (!p.second) {
        int suffix_num = p.first->second;
        this->name_cnts[name] = suffix_num + 1;
        suffix_str = "_" + to_string(suffix_num);
    } else {
        suffix_str = "";
    }
    Op *op = new Op{type, prec_type,         in_deps, out_deps,
                    args, name + suffix_str, gran_lev};
    assert(op != nullptr);
    this->ops_storage.emplace_back(op);
    for (auto &tns : in_deps) {
        // If an input tensor is not generated by another op,
        // the buffer for this tensor may store external input data,
        // so we set the buffer to be immutable.
        if (this->get_gen_op(tns) == nullptr) {
            tns->buf->immutable = true;
        }
        this->ref_ops[tns].insert(op);
    }
    for (auto &tns : out_deps) {
        this->gen_op[tns] = op;
    }
    return op;
}

// Returns the latest-declared operator that has the given tensor as its output.
const Op *Model::get_gen_op(Tensor *tns) const
{
    auto search = this->gen_op.find(tns);
    if (search == this->gen_op.end()) {
        return nullptr;
    }
    return search->second;
}

// Returns the set of operators that have the given tensor as one of their
// inputs.
const std::set<Op *> &Model::get_ref_ops(Tensor *tns) const
{
    auto search = this->ref_ops.find(tns);
    if (search == this->ref_ops.end()) {
        LOGERR("Not an existing tensor.");
    }
    return search->second;
}

// Returns true if the given tensor is not an input of any operator.
bool Model::is_no_ref(Tensor *tns) const
{
    auto search = this->ref_ops.find(tns);
    if (search == this->ref_ops.end()) {
        return true;
    }
    return false;
}

void to_json(nlohmann::json &j, const Model &model)
{
    j = nlohmann::json{
        {"tensors", vector<Tensor>{}},
        {"ops", vector<Op>{}},
    };
    for (auto &pt : model.get_tensors()) {
        j.at("tensors").emplace_back(*pt);
    }
    for (auto &po : model.get_ops()) {
        j.at("ops").emplace_back(*po);
    }
}
void from_json(const nlohmann::json &j, Model &model)
{
}

} // namespace ark
